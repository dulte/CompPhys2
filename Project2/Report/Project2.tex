\documentclass[a4paper, 10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{a4wide}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvips]{epsfig}
\usepackage[toc,page]{appendix}
\usepackage[T1]{fontenc}
\usepackage{cite} % [2,3,4] --> [2--4]
\usepackage{shadow}
\usepackage{hyperref}
\usepackage{titling}
\usepackage{marvosym }
\usepackage{physics}

\usepackage{subcaption}
\usepackage[noabbrev]{cleveref}
\usepackage{wasysym}
\usepackage{changepage}


\renewcommand{\topfraction}{.85}
\renewcommand{\bottomfraction}{.7}
\renewcommand{\textfraction}{.15}
\renewcommand{\floatpagefraction}{.66}
\renewcommand{\dbltopfraction}{.66}
\renewcommand{\dblfloatpagefraction}{.66}
\setcounter{topnumber}{9}
\setcounter{bottomnumber}{9}
\setcounter{totalnumber}{20}
\setcounter{dbltopnumber}{9}


\setlength{\droptitle}{-10em}   % This is your set screw

\setcounter{tocdepth}{2}

\lstset{language=c++}
\lstset{alsolanguage=[90]Fortran}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{red}\bfseries}
\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\title{FYS4411 - Project 2\\
	The restricted Boltzmannn machine applied to the quantum many body problem}
\author{Daniel Heinesen$^1$, Gunnar Lange$^2$ \& Aram Salihi$^2$\\
	\small $^1$Department of Theoretical Astrophysics, University of Oslo, N-0316 Oslo, Norway\\
	\small $^2$Department of Physics, University of Oslo, N-0316 Oslo, Norway}
\begin{document}
	\maketitle
	\begin{abstract}
	\begin{center}
	 NICE ABSTRACT
\end{center}
	\end{abstract}
	\newpage
	\tableofcontents
	\newpage
	\section{Introduction}
	COOL INTRO
	\section{Theory}
	Our system of $P$ electrons confined in an isotropic harmonic oscillator potential is described by the Hamiltonian:
	\begin{equation}\label{eq:Hamiltonian}
	H=\sum_{i=1}^P \left(-\frac{1}{2}\nabla_i^2 +\frac{1}{2}\omega^2 r_i^2\right)+\sum_{i=1}^P \sum_{j=1}^{i}\frac{1}{r_{ij}}
	\end{equation}
	Where this is in natural units, i.e. $\hbar=c=e=m_e=1$. Here $\omega$ is the frequency of the oscillator trap and $1/r_{ij}$ is the distance between particle $i$ and $j$. The first term describes the interaction of the electrons with the potential, and we refer to it as the noninteracting part. The second term describes the Coloumb interaction between the electrons, and we refer to it as the interacting part.\\
	\linebreak
	We now wish to find the energy, $E$ of our system by solving the time-independent Schr√∂dinger equation:
	\begin{equation}\label{eq:time_depenedent_schrodinger}
	H\Psi = E\Psi
	\end{equation}
	\subsection{Analytic expression}
	We are chiefly interested in the ground-state energy, as this is the state that our system will fall into if unperturbed by e.g. thermal excitation. If we ignore the interacting part, then we have an analytic solution to the ground-state energy for two electrons, given by:
	\begin{equation}
	\Psi(\mathbf{r}_1, \mathbf{r}_2)=C\exp\left(-\omega\frac{\boldsymbol{r_1}^2+\boldsymbol{r_2}^2}{2}\right)
	\end{equation}
	Where $C$ is a normalization constant. Furthermore, the ground-state energy in such a harmonic oscillator is, according to \textbf{REFERENCE}, given by:
	\begin{equation}\label{eq:Ground-state-energy}
	E=P\frac{D}{2}
	\end{equation}
	Where $D$ is the number of dimensions and $P$ is the number of particles.
	\textbf{FINISH - DISCUSS SPIN, TWO PARTICLES, ANALYTIC SOLUTION FOR SIMPLE INTERACTION}
	\subsection{The variational principle}
	 In the more complicated case of interacting fermions,  
	 We achieve such an upper bound by invoking the variational principle, as formulated in \textbf{GRIFFITHS}, which states that:
	\begin{equation}\label{eq:Variational_Principle}
	E_0\leq \frac{\langle \Psi_T | H |\Psi_T \rangle}{\langle \Psi_T | \Psi_T \rangle}
	\end{equation} 
	Where $E_0$ is the ground-state energy and $\Psi_T$ is any (not necessarily normalized) wavefunction. Our approach is therefore to choose a trial wavefunction, $\Psi_T$, and the compute $E'=\langle \Psi_T | H |\Psi_T\rangle$ by using a variational Monte Carlo approach. The variational method then guarantees that our approximation to the energy, $E'$ will be larger than the ground state energy $E_0$. 
	\subsection{Neural-network quantum states and Restricted Boltzmann Machines}
	We follow the approach outlined by \textbf{REFERENCE} to determine our trial wavefunction. They choose to interpret the problem of solving equation \ref{eq:time_depenedent_schrodinger} as a multi-dimensional minimization and feature extraction problem. We therefore employ a neural network to find the optimal wavefunction. Thus, we represent our wavefunction by a set of parameters, which we then seek to optimize. We choose to follow \textbf{REFERENCE} and choose a restricted Boltzmann machine (RBM) appproach to this neural network.\\
	\linebreak
	Restricted Boltzmann Machines (RBM) are described in great detail in the literature, see e.g. \textbf{REFERENCE}. We therefore only outline how to apply them. The basic idea consists of setting up a vector of $M$ visible nodes $\mathbf{X}$, which corresponds to the position states of our system (with one state per particle, $P$ and dimension, $D$, i.e. $M=PD$), and a vector of $N$ hidden nodes, $\mathbf{H}$ which correspond to some feature of our system \textbf{DANIEL CHECK THIS PLEASE}. Each of these nodes have weights associated with them. Furthermore, there are also weights coupling the hidden nodes to the visible nodes, but there are no connections between nodes in the same layer (hence \textit{restricted} Boltzmann machine). We then model the probability distribution for our particles, given a position $\mathbf{X}$ and a set of hidden nodes $\mathbf{H}$. by a Boltzmann distribution function of the form:
	\begin{equation}
	F_{rbm}(\mathbf{X}, \mathbf{H})=\frac{1}{Z}\exp \left(-\frac{1}{T_0}E(\mathbf{X},\mathbf{H})\right)
	\end{equation}
	Where $Z$ is a normalization constant, $T_0$ is some reference temperature (set to $1$ in what follows), and the energy is chosen to represent a continuous function of the position, given as:
	\begin{equation}
	E(\mathbf{X}, \mathbf{H})=\sum_{i=1}^{M}\frac{\left(X_i-a_i\right)^2}{2\sigma^2}-\sum_{j=1}^N b_j H_j -\sum_{i=1}^{N}\sum_{j=1}^{M}\frac{X_iw_{ij}H_j}{\sigma^2}
	\end{equation}
	Where $a_i$ are the weights associated with the visible nodes, $b_i$ are the weights associated with the visible nodes and $w_{ij}$ are the weights of the coupling between the visible and hidden nodes. The $\sigma$ models the spread of the energy.\\
	\linebreak
	As our wavefunction is a function of the position coordinates only, we choose to interpret the marginal distribution of the visible nodes as our wavefunction. This marginal distribution is given by:
	\begin{equation}
	F_{rbm}(\mathbf{X})=\sum_{h_i \in \mathbf{H}}F_{rbm}(\mathbf{X}, \mathbf{H})
	\end{equation}
	Which gives, after some algebra done in \textbf{REFERENCE}, our wavefunction as:
	\begin{equation}\label{eq:Wavefunction}
	\Psi_T(\mathbf{X})=F_{rbm}(\mathbf{X})=\frac{1}{Z}\exp \left(-\sum_{i=1}^M \frac{(X_i-a_i)^2}{2\sigma^2}\right)\prod_{j=1}^{N}\left(1+\exp\left(b_j+\sum_{i=1}^M \frac{X_iw_{ij}}{\sigma^2} \right)\right)
	\end{equation}
	This is therefore our trial wavefunction that we use in equation \ref{eq:Variational_Principle} to find an upper bound on the ground-state energy of our system. We seek to optimize this wavefunction with respect to the weights $a_i$, $b_i$ and $w_{ij}$. We also investigate how this energy behaves as a function of $\sigma$, as well as for different dimensions $D$ and particles $P$.
	\textbf{TALK ABOUT IF THIS CAN BE EXACT FOR NONINTERACTING}
	\subsection{Variational Monte Carlo methods}
	Having determined our trial wavefunction, it remains to use the variational principle to estimate the ground state energy. Our presentation here will closely follow that of \textbf{REFERENCE}. We use a variational Monte Carlo approach to estimate the inner product given in equation \ref{eq:Variational_Principle}. To do so, we consider the integrand function of this integral, given by:
	\begin{equation}
	P(\mathbf{X})=\frac{|\Psi_T|^2}{\int d\mathbf{X}|\Psi_T|^2}
	\end{equation}
	Where the integral is to be carried out over all particles and dimension (i.e. $M$ times). If we now define the local energy (by solving equation \ref{eq:time_depenedent_schrodinger} for $E$) as:
	\begin{equation}\label{eq:Local_energy}
	E_L(\mathbf{X})=\frac{1}{\Psi_T}H\Psi_T
	\end{equation}
	Then the expectation value of the Hamiltonian (which gives the energy) is given by:
	\begin{equation}\label{eq:energy_in_state_space}
	E_0 \leq E[H]=\int d\mathbf{X}P(\mathbf{X})E_L(\mathbf{X})\approx \frac{1}{K}\sum_{i=1}^{K}E_{L,i}
	\end{equation}
	Thus, we can get an approximation to the ground state energy if we can efficiently compute the local energy, and efficiently sample our space close to the optimal energy.
	\subsection{The local energy}
	The local energy is defined in equation \ref{eq:Local_energy}, with the Hamiltonian given in equation \ref{eq:Hamiltonian}. Note that the most complicated part of this expression is the second derivative of the wavefunction. We derive an analytic expression for this is in appendix \textbf{APPENDIX}. The local energy is then given by:
	\begin{equation}
	\begin{split}
	\frac{1}{\Psi_T}H\Psi_T=\sum_{k=1}^M-\frac{1}{2}\left[\frac{a_k-X_k}{\sigma^2}+\sum_{j=1}^N \frac{w_{kj}}{\sigma^2 \left(1+ \exp \left(-b_j-\sum_{i=1}^{M} \frac{X_iw_{ij}}{\sigma^2}\right)\right)}\right]^2\\
	+\sum_{k=1}^M\frac{1}{2\sigma^2}-\frac{1}{2}\sum_{k=1}^M\sum_{j=1}^N \frac{w_{kj}^2 \exp \left(-b_j-\sum_{i=1}^{M}\frac{X_iw_{ij}}{\sigma^2} \right)}{\sigma^4\left(1+ \exp \left(-b_j-\sum_{i=1}^{M} \frac{X_iw_{ij}}{\sigma^2}\right)\right)^2}+\frac{1}{2}\sum_{i=1}^M\omega^2 X_i^2+\sum_{i=1}^P \sum_{j=1}^i \frac{1}{r_{ij}}
	\end{split}
	\end{equation}
	\subsection{Sampling the position space}
	To make sense of equation \ref{eq:energy_in_state_space}, we must sample our position space, to get a good estimate for the local energy of the system. We employ three different sampling techniques.	
	\subsubsection{The Metropolis algorithm}
	The Metropolis algorithm is described in some detail \textbf{REFERENCE}. Note that the wavefunction gives the probability of finding the particle at point in position space. We will therefore get the most reliable estimates for our local energy if we sample where the wavefunction (and thus the probability distribution) is large, and we avoid wasting CPU cycles. The Metropolis algorithm is one way to achieve this. This is achieved by proposing to move a single particle according to:
	\begin{equation}\label{eq:metropolis_algorithm_step_size}
	\boldsymbol{r}_{\mathrm{i,new}}=\boldsymbol{r}_{\mathrm{i}}+\boldsymbol{\xi}dx
	\end{equation}
	Where $\boldsymbol{r}_{\mathrm{i}}$ is the position of particle $i$, $dx$ is a chosen step size and $\boldsymbol{\xi}$ is a D-dimensional vector of random numbers distributed as $\xi_i \sim 0.5\cdot \mathcal{N}(0,1)$. To find out whether we should sample local energy at this new position, we compute the ratio:
	\begin{equation}\label{eq:transition_probability}
	w=\frac{P(\boldsymbol{X}_{\mathrm{new}})}{P(\boldsymbol{X})}=\frac{|\Psi(\boldsymbol{X}_{\mathrm{new}})|^2}{|\Psi(\boldsymbol{X})|^2}
	\end{equation}
	Where $\boldsymbol{X}_{\mathrm{new}}$ is the position of the particles after updating the position of particle $i$ according to \ref{eq:metropolis_algorithm_step_size}. Note that this ratio determines if the particle is more likely to be found at the position $\boldsymbol{X}_{\mathrm{new}}$ than at position $\boldsymbol{X}$. The move is only accepted if $w\geq \theta$, where $\theta\in[0,1]$ is a uniformly distributed random variable. Thus, if the probability of finding the particle at $\mathbf{X}_{\mathrm{new}}$ is larger than the probability of finding the particle at $\mathbf{X}$, then we accept the move. If not, then the larger the energy difference the less probable it is that the move is accepted. As such, we should eventually end up moving fairly close to the most likely position, without getting stuck in it.
	\subsubsection{The Metropolis-Hasting alogrithm/importance sampling}
	We compare the Metropolis algorithm to an alternative, known as importance sampling. This is described in further detail in \textbf{REFERENCE}. Note that the Metropolis sampling described in the previous section picks positions at random, and as such does not have a preference for proposing steps that move the system to a region of high probability density (given by $|\Psi|^2$). As such, some CPU cycles will be wasted on position that a very small probability of occurring. Importance sampling seeks to alleviate this problem by introducing a quantum force, which "pushes" the system in the direction of high probability. This force is, on particle $i$ given by:
	\begin{equation}
	F_i(\boldsymbol{X})=\frac{2}{\Psi_T}\nabla_i \Psi_T=2\nabla_i \log \Psi_T
	\end{equation}
	We compute this in appendix \textbf{APPENDIX} and show that this force is given by:
	\begin{equation}
	F_i(\boldsymbol{X})=2\sum_{k=1}^D \left(\frac{a_k-X_k}{\sigma^2}+\sum_{j=1}^N \frac{w_{kj}}{\sigma^2 \left(1+ \exp \left(-b_j-\sum_{i=1}^{M} \frac{X_iw_{ij}}{\sigma^2}\right)\right)}\right)
	\end{equation}
	The analysis in \textbf{REFERENCE} shows that the correct way to include this in our algorithm is by multiplying the transition probability $w$ defined in equation \ref{eq:transition_probability} by a Green's function given by:
	\begin{equation}
	G(\mathbf{X}, \mathbf{X}')=\frac{1}{(4D\Delta t)^{3/2}}\sum_{i=1}^M \exp \left(-\frac{(X_i-X_i'-D\Delta t F_i(\mathbf{X}'))^2}{4D\Delta t}\right)
	\end{equation}
	And the modified transition probability is given by:
	\begin{equation}
	w_{\mathrm{importance}}=\frac{|\Psi(\boldsymbol{X}_{\mathrm{new}})|^2}{|\Psi(\boldsymbol{X})|^2}\frac{G(\boldsymbol{X}, \boldsymbol{X}_{\mathrm{new}})}{G(\boldsymbol{X}_{new}, \boldsymbol{X})}
	\end{equation}
	This transition probability is implemented exactly as in the previous section. However, we use a slightly different initialization and we draw the next step, $\boldsymbol{\xi}dx$ in a slightly different way when using importance sampling. This is described in further detail in \textbf{SECTION}.
	\subsubsection{Gibbs Sampling}
	\subsection{Optimizing our wavefunction in parameter space}
	For our neural network to be successful, we must be able to minimize the energy of our wavefunction. This is done by varying the parameters $a_i$, $b_i$ and $w_{ij}$, and computing the local energy. The variational method then guarantees that the lowest energy we find will be larger than the ground state energy.
	
	
	
	\section{Methods}\label{Method_section}
	MUCHO METHOD
	\section{Results}
	GROUNDBREAKING RESULTS
	\section{Discussion}
	EXCELLENT DISCUSSION
	\section{Conclusion}
	BRILLIANT CONCLUSION
	\subsection{Outlook}
	MAGNIFICIENT OUTLOOK
	\bibliographystyle{apalike}
	\bibliography{Project1}
	\pagebreak
	\begin{appendices}
	\section{Finding the derivatives}\label{ap:finding_derivatives}
	In this section, we derive the expressions for the various derivatives stated in section \textbf{SECTION}. We begin with the local energy.
	\subsection{The local energy}
	The local energy is given by:
	\begin{equation}
	E_L=\frac{1}{\Psi}\hat{H}\Psi= \sum_{i=1}^N \left(-\frac{1}{2\Psi}\nabla_i^2 \Psi + \frac{1}{2}\omega^2 r_i^2\right)+\sum_{i<j} \frac{1}{r_{ij}}
	\end{equation}
	Thus, we must compute:
	\begin{equation}
	\frac{1}{\Psi}\nabla_i^2 \Psi
	\end{equation}
	This may be rewritten as:
	\begin{equation}\label{eq:log_expression_for_trial_wavefunction}
	\frac{1}{\Psi}\nabla \left(\Psi \frac{1}{\Psi} \nabla\Psi\right)=\left(\frac{1}{\Psi}\nabla \Psi\right)^2+\nabla \left(\frac{1}{\Psi}\nabla \Psi \right)=\left[\nabla \log \Psi\right]^2 + \nabla^2 \log \Psi
	\end{equation}
	The logarithm of our trial wavefunction is given by:
	\begin{equation}\label{eq:logarithm_of_wavefunction}
	\log \Psi = -\log Z -\sum_{i=1}^M \left(\frac{(X_i-a_i)^2}{2\sigma^2}\right)+\sum_{j=1}^N \log \left(1+\exp \left(b_j+\sum_{i=1}^{M} \frac{X_iw_{ij}}{\sigma^2}\right)\right)
	\end{equation}
	The derivative with respect to one coordinate is now given by:
	\begin{equation}\label{eq:first_derivative_log_psi}
	\begin{split}
	\frac{\partial \log \Psi}{\partial X_k}=\frac{(a_k-X_k)}{\sigma^2}+\sum_{j=1}^N \frac{w_{kj} \exp \left(b_j+\sum_{i=1}^{M} \frac{X_iw_{ij}}{\sigma^2}\right)}{\sigma^2 \left(1+\exp \left(b_j+\sum_{i=1}^{M} \frac{X_iw_{ij}}{\sigma^2}\right)\right)}\\
	=\frac{a_k-X_k}{\sigma^2}+\sum_{j=1}^N \frac{w_{kj}}{\sigma^2 \left(1+ \exp \left(-b_j-\sum_{i=1}^{M} \frac{X_iw_{ij}}{\sigma^2}\right)\right)}
	\end{split}
	\end{equation}
	Whereas the second derivative is:
	\begin{equation}\label{eq:second_derivative_log_psi}
	\frac{\partial^2 \log \Psi}{\partial X_k^2}=-\frac{1}{\sigma^2}+\sum_{j=1}^N \frac{w_{kj}^2 \exp \left(-b_j-\sum_{i=1}^{M}\frac{X_iw_{ij}}{\sigma^2} \right)}{\sigma^4\left(1+ \exp \left(-b_j-\sum_{i=1}^{M} \frac{X_iw_{ij}}{\sigma^2}\right)\right)^2}
	\end{equation}
	The local energy can now be found by using equation \ref{eq:log_expression_for_trial_wavefunction}, and inserting equation \ref{eq:first_derivative_log_psi} and equation \ref{eq:second_derivative_log_psi}.
	\subsection{The derivatives with respect to the parameters}
	For our optimization method, we require:
	\begin{equation}
	\frac{1}{\Psi}\frac{\partial \Psi}{\partial \alpha_k}=\frac{\partial }{\partial \alpha_k}\log \Psi
	\end{equation}
	Where $\alpha_k$ is any of the variational parameters $a$, $b$ or $w$. These derivatives are given by:
	\begin{equation}
	\frac{\partial \log \Psi }{\partial a_k}=\frac{X_k-a_k}{\sigma^2}
	\end{equation}
	\begin{equation}
	\frac{\partial \log \Psi}{\partial b_k}=\frac{\exp \left(b_k+\sum_{i=1}^M \frac{X_iw_{ik}}{\sigma^2}\right)}{1+\exp \left(b_k+\sum_{i=1}^M \frac{X_iw_{ik}}{\sigma^2}\right)}=\frac{1}{1+\exp \left(-b_k-\sum_{i=1}^M \frac{X_i w_{ik}}{\sigma^2}\right)}
	\end{equation}
	Finally, the derivative with respect to the weights, $w_{kl}$ is given by:
	\begin{equation}
	\frac{\partial \log \Psi}{\partial w_{kl}}=\frac{X_k\exp\left(b_l+\sum_{i=1}^M \frac{X_iw_{il}}{\sigma^2}\right)}{\sigma^2\left(1+\exp \left( b_l + \sum_{i=1}^M \frac{X_iw_{il}}{\sigma^2}\right)\right)}=\frac{X_k}{\sigma^2\left(1+\exp \left(-b_l-\sum_{i=1}^M \frac{X_iw_{il}}{\sigma^2}\right)\right)}
	\end{equation}
	\subsection{The derivatives with Gibbs sampling}
	In Gibbs sampling, we represent the wavefunction as $\Psi=\sqrt{F_{rbm}}$, instead of $\Psi=F_{rbm}$. Note, however, that we only ever differentiate the logarithm of the wavefunction. As $\log \sqrt{\Psi}=\frac{1}{2}\log \Psi$, however, this is only a marginal change. Specifically:
	\begin{equation}
	E_L=[\nabla \log \sqrt{\Psi}]^2 + \nabla^2 \log \sqrt{\Psi} =\frac{1}{4}[\nabla \log \Psi]^2 +\frac{1}{2}\nabla^2 \log \Psi
	\end{equation}
	\begin{equation}
	\frac{\partial }{\partial \alpha_k}\log \sqrt{\Psi}=\frac{1}{2}\frac{\partial }{\partial \alpha_k}\Psi
	\end{equation}
	\begin{equation}
	\frac{\partial }{\partial b_k}\log \sqrt{\Psi}=\frac{1}{2}\frac{\partial }{\partial b_k}\log \Psi 
	\end{equation}
	\begin{equation}
	\frac{\partial}{\partial w_{kl}}\log \sqrt{\Psi}=\frac{1}{2}\frac{\partial}{\partial w_{kl}}\log \Psi
	\end{equation}
	Where the derivatives on the right-hand side are given in the previous section.
\section{Deriving the spin of the ground state wavefunction}
Consider the unperturbed wavefunction for the ground state of the
two electron system, given by:
\begin{align}
\psi(\boldsymbol{r}_{1},\boldsymbol{r}_{2}) =
C\exp\left(-\omega\left(r_{1}^{2} + r_{2}^{2}\right)/2\right)
\end{align}
Since this unperturbed wavefunction are symmetric under permutation of coordinate.
Pauli exclusion principle then states that the spin wavefunction must then be
a antisymmetric function in order to achieve
total spin $0$ under permutation of coordinate.
This wave function must then be singlet, and will look like:
\begin{align}
\ket{\psi} = \frac{1}{\sqrt{2}}\left(\ket{\uparrow} - \ket{\downarrow}\right)
\end{align}
We will now apply the spin operator $\boldsymbol{S}^{2}$ on this spin state to prove that
total spin is $0$:
\begin{align}
\boldsymbol{S}^{2}\ket{\psi} = \frac{1}{\sqrt{2}}\left(
\underbrace{\boldsymbol{S}^{2}\ket{\uparrow}}_{= \frac{3}{4}\hbar^{2}} -
\underbrace{\boldsymbol{S}^{2}\ket{\downarrow}}_{= \frac{3}{4}\hbar^{2}}\right)
= 0
\end{align}
	\end{appendices}
\end{document}